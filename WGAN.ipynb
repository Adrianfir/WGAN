{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9430c18a",
   "metadata": {},
   "source": [
    "# WGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5d57a",
   "metadata": {},
   "source": [
    "## Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96d3f0b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_config' from 'tensorflow.python.eager.context' (C:\\Users\\adria\\anaconda3\\envs\\adrian\\lib\\site-packages\\tensorflow\\python\\eager\\context.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10812\\347101878.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2DTranspose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adrian\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adrian\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\adrian\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_config' from 'tensorflow.python.eager.context' (C:\\Users\\adria\\anaconda3\\envs\\adrian\\lib\\site-packages\\tensorflow\\python\\eager\\context.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras \n",
    "from keras.layers import Flatten, Dense, Reshape, Dropout, Input, BatchNormalization, Conv2D, Conv2DTranspose\n",
    "from keras import regularizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.datasets import fashion_mnist as mnist\n",
    "from keras.models import load_model\n",
    "from keras.initializers import RandomNormal\n",
    "from keras import backend\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc1ee8",
   "metadata": {},
   "source": [
    "## Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81d75525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Input_Data():\n",
    "    \n",
    "    (Xtrain,_) , (_,_) = mnist.load_data()\n",
    "\n",
    "    #     Xtrain = (Xtrain.astype('float32')/127.50) - 1\n",
    "    Scaler = StandardScaler()\n",
    "\n",
    "    Xtrain_shape = Xtrain.shape\n",
    "\n",
    "    Xtrain = pd.DataFrame(Xtrain.reshape(Xtrain_shape[0],-1))\n",
    "    Xtrain = Scaler.fit_transform(Xtrain)\n",
    "    Xtrain = np.array(Xtrain)\n",
    "    Xtrain = np.reshape(Xtrain,[Xtrain_shape[0],Xtrain_shape[1],Xtrain_shape[2]])\n",
    "    Xtrain = Xtrain[...,np.newaxis]\n",
    "    \n",
    "    return Xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c756e83",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce140ab",
   "metadata": {},
   "source": [
    "### Generative Network - Discriminative Network - GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad4450aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:   \n",
    "    \n",
    "    ### Generative Network\n",
    "    def build_Gen(self,z_dim):\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "        \n",
    "        self.Gen = Sequential()\n",
    "    \n",
    "        self.Gen.add(Dense(128*7*7,input_dim=z_dim, kernel_initializer=init))\n",
    "        self.Gen.add(LeakyReLU(0.2))\n",
    "        self.Gen.add(Reshape((7,7,128)))\n",
    "        \n",
    "        self.Gen.add(Conv2DTranspose(64,kernel_size=3,strides=2, padding='same',kernel_initializer=init))\n",
    "        self.Gen.add(BatchNormalization())\n",
    "        self.Gen.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        self.Gen.add(Conv2DTranspose(32,kernel_size=3, strides=2, padding='same',kernel_initializer=init))\n",
    "        self.Gen.add(BatchNormalization())\n",
    "        self.Gen.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        self.Gen.add(Conv2DTranspose(1,kernel_size=3,strides=1, padding='same', activation = 'tanh',\\\n",
    "                                     kernel_initializer=init))\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    ### Discriminative Network\n",
    "    def build_Dis(self,data_size):\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "        \n",
    "        self.Dis = Sequential()\n",
    "        \n",
    "        self.Dis.add(Conv2D(32, kernel_size = 3, strides=2, input_shape = data_size, padding = 'same',kernel_initializer=init))\n",
    "        self.Dis.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        self.Dis.add(Conv2D(64, kernel_size = 3, strides=2, padding = 'same', kernel_initializer=init))\n",
    "        self.Dis.add(BatchNormalization())\n",
    "        self.Dis.add(LeakyReLU(alpha = 0.2))   \n",
    "\n",
    "        self.Dis.add(Conv2D(128, kernel_size = 3, strides=2, padding = 'same', kernel_initializer=init))\n",
    "        self.Dis.add(BatchNormalization())\n",
    "        self.Dis.add(LeakyReLU(alpha = 0.2)) \n",
    "        \n",
    "        self.Dis.add(Flatten())\n",
    "        \n",
    "        self.Dis.add(Dense(1))\n",
    "#         self.Dis.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    ### GAN Network\n",
    "    def build_GAN(self):\n",
    "        self.GAN = Sequential()\n",
    "    \n",
    "        self.GAN.add(self.Gen)\n",
    "        self.GAN.add(self.Dis)\n",
    "    \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d6bc3",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29d5d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_results(model,z_dim):\n",
    "    \n",
    "    z = np.random.normal(0, 1, (16, z_dim))\n",
    "    gen_imgs = model.Gen.predict(z)\n",
    "    gen_imgs = 0.5*gen_imgs + 0.5\n",
    "\n",
    "    fig,axs = plt.subplots(4,4,figsize=(4,4),sharey=True,sharex=True)\n",
    "\n",
    "    cnt=0\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axs[i, j].imshow(gen_imgs[cnt,:,:,0],cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt+=1\n",
    "\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cce223",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff75f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(batch_size, Xtrain, z_dim, Info, Iters, Interval):\n",
    "    \n",
    "    Real_ideal = np.ones((batch_size,1))\n",
    "    Fake_ideal = -np.ones((batch_size,1))\n",
    "    \n",
    "    for Iter in range(Iters):\n",
    "        ## Discrimintive Network Training\n",
    "\n",
    "        \n",
    "        for k in range(5):\n",
    "                 \n",
    "            Xtrain_sample = Xtrain[np.random.randint(0,Xtrain.shape[0],batch_size)]\n",
    "            Z = np.random.normal(0,1,(batch_size,z_dim))      \n",
    "            Fake_Data_by_GenerativeNet = model.Gen.predict(Z)\n",
    "            \n",
    "            Dis_loss_Real = model.Dis.train_on_batch(Xtrain_sample,Real_ideal)\n",
    "            Dis_loss_Fake = model.Dis.train_on_batch(Fake_Data_by_GenerativeNet,Fake_ideal)\n",
    "        \n",
    "            Dis_loss, Dis_accuracy = 0.5*(np.add(Dis_loss_Real,Dis_loss_Fake))\n",
    "        \n",
    "        \n",
    "        ## Generative Network Training\n",
    "        Z = np.random.normal(0,1,(batch_size,z_dim))\n",
    "        Gen_loss = model.GAN.train_on_batch(Z,Fake_ideal)\n",
    "        \n",
    "        if (Iter+1) % Interval == 0:\n",
    "            Info.losses.append((Dis_loss,Gen_loss))\n",
    "            Info.accuracies.append(np.multiply(100,Dis_accuracy))\n",
    "            Info.iteration_checks.append(Iter+1)\n",
    "            \n",
    "            print(\"%d [Dis_loss: %f , Dis_accuracy: %.2f] [Gen_loss: %f]\" %\n",
    "                  (Iter+1,Dis_loss,np.multiply(100.0,Dis_accuracy),Gen_loss))            \n",
    "            \n",
    "            show_results(model,z_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91d4bc",
   "metadata": {},
   "source": [
    "## Wasserstein Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e68beb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wasserstein_loss (y_True, y_Pred):\n",
    "    return backend.mean(y_True*y_Pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b115020",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c4783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [Dis_loss: -35.214312 , Dis_accuracy: 0.00] [Gen_loss: -0.267809]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\anaconda3\\envs\\adrian\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 [Dis_loss: -65.136288 , Dis_accuracy: 0.00] [Gen_loss: -2.745317]\n",
      "30 [Dis_loss: -94.575283 , Dis_accuracy: 0.00] [Gen_loss: -8.716883]\n",
      "40 [Dis_loss: -124.920319 , Dis_accuracy: 0.00] [Gen_loss: -19.322800]\n"
     ]
    }
   ],
   "source": [
    "class info:\n",
    "    \n",
    "    def __init__(self,losses=[],accuracies=[],iteration_checks=[]):\n",
    "        \n",
    "        self.losses = losses\n",
    "        self.accuracies = accuracies\n",
    "        self.iteration_checks = iteration_checks\n",
    "        \n",
    "Xtrain = Input_Data()\n",
    "data_size = Xtrain.shape[1:]\n",
    "batch_size = 64\n",
    "z_dim = 100\n",
    "\n",
    "model  = GAN()\n",
    "\n",
    "model.build_Dis(data_size)\n",
    "model.Dis.compile (loss = Wasserstein_loss,\n",
    "               optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2 = 0.999),\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "model.build_Gen(z_dim)\n",
    "# model.Gen.compile(loss = 'binary_crossentropy',\n",
    "#                optimizer = Adam())\n",
    "\n",
    "model.Dis.trainable = False\n",
    "\n",
    "model.build_GAN()\n",
    "model.GAN.compile(loss = Wasserstein_loss,\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2 = 0.999))\n",
    "\n",
    "    \n",
    "Info = info([],[],[])\n",
    "Train(batch_size, Xtrain, z_dim, Info, Iters = 20000, Interval = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
