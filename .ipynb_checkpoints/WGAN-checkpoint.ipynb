{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9430c18a",
   "metadata": {},
   "source": [
    "# WGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5d57a",
   "metadata": {},
   "source": [
    "## Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d3f0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras \n",
    "from keras.layers import Flatten, Dense, Reshape, Dropout, Input, BatchNormalization, Conv2D, Conv2DTranspose\n",
    "from keras import regularizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.datasets import fashion_mnist as mnist\n",
    "from keras.models import load_model\n",
    "from keras.initializers import RandomNormal\n",
    "from keras import backend\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc1ee8",
   "metadata": {},
   "source": [
    "## Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d75525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Input_Data():\n",
    "    \n",
    "    (Xtrain,_) , (_,_) = mnist.load_data()\n",
    "\n",
    "    #     Xtrain = (Xtrain.astype('float32')/127.50) - 1\n",
    "    Scaler = StandardScaler()\n",
    "\n",
    "    Xtrain_shape = Xtrain.shape\n",
    "\n",
    "    Xtrain = pd.DataFrame(Xtrain.reshape(Xtrain_shape[0],-1))\n",
    "    Xtrain = Scaler.fit_transform(Xtrain)\n",
    "    Xtrain = np.array(Xtrain)\n",
    "    Xtrain = np.reshape(Xtrain,[Xtrain_shape[0],Xtrain_shape[1],Xtrain_shape[2]])\n",
    "    Xtrain = Xtrain[...,np.newaxis]\n",
    "    \n",
    "    return Xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c756e83",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce140ab",
   "metadata": {},
   "source": [
    "### Generative Network - Discriminative Network - GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4450aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:   \n",
    "    \n",
    "    ### Generative Network\n",
    "    def build_Gen(self,z_dim):\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "        \n",
    "        self.Gen = Sequential()\n",
    "    \n",
    "        self.Gen.add(Dense(128*7*7,input_dim=z_dim, kernel_initializer=init))\n",
    "        self.Gen.add(LeakyReLU(0.2))\n",
    "        self.Gen.add(Reshape((7,7,128)))\n",
    "        \n",
    "        self.Gen.add(Conv2DTranspose(64,kernel_size=3,strides=2, padding='same',kernel_initializer=init))\n",
    "        self.Gen.add(BatchNormalization())\n",
    "        self.Gen.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        self.Gen.add(Conv2DTranspose(32,kernel_size=3, strides=2, padding='same',kernel_initializer=init))\n",
    "        self.Gen.add(BatchNormalization())\n",
    "        self.Gen.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        self.Gen.add(Conv2DTranspose(1,kernel_size=3,strides=1, padding='same', activation = 'tanh',\\\n",
    "                                     kernel_initializer=init))\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    ### Discriminative Network\n",
    "    def build_Dis(self,data_size):\n",
    "        \n",
    "        init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "        \n",
    "        self.Dis = Sequential()\n",
    "        \n",
    "        self.Dis.add(Conv2D(32, kernel_size = 3, strides=2, input_shape = data_size, padding = 'same',kernel_initializer=init))\n",
    "        self.Dis.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        self.Dis.add(Conv2D(64, kernel_size = 3, strides=2, padding = 'same', kernel_initializer=init))\n",
    "        self.Dis.add(BatchNormalization())\n",
    "        self.Dis.add(LeakyReLU(alpha = 0.2))   \n",
    "\n",
    "        self.Dis.add(Conv2D(128, kernel_size = 3, strides=2, padding = 'same', kernel_initializer=init))\n",
    "        self.Dis.add(BatchNormalization())\n",
    "        self.Dis.add(LeakyReLU(alpha = 0.2)) \n",
    "        \n",
    "        self.Dis.add(Flatten())\n",
    "        \n",
    "        self.Dis.add(Dense(1))\n",
    "#         self.Dis.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    ### GAN Network\n",
    "    def build_GAN(self):\n",
    "        self.GAN = Sequential()\n",
    "    \n",
    "        self.GAN.add(self.Gen)\n",
    "        self.GAN.add(self.Dis)\n",
    "    \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d6bc3",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d5d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_results(model,z_dim):\n",
    "    \n",
    "    z = np.random.normal(0, 1, (16, z_dim))\n",
    "    gen_imgs = model.Gen.predict(z)\n",
    "    gen_imgs = 0.5*gen_imgs + 0.5\n",
    "\n",
    "    fig,axs = plt.subplots(4,4,figsize=(4,4),sharey=True,sharex=True)\n",
    "\n",
    "    cnt=0\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axs[i, j].imshow(gen_imgs[cnt,:,:,0],cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            cnt+=1\n",
    "\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cce223",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff75f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(batch_size, Xtrain, z_dim, Info, Iters, Interval):\n",
    "    \n",
    "    Real_ideal = np.ones((batch_size,1))\n",
    "    Fake_ideal = -np.ones((batch_size,1))\n",
    "    \n",
    "    for Iter in range(Iters):\n",
    "        ## Discrimintive Network Training\n",
    "\n",
    "        \n",
    "        for k in range(5):\n",
    "                 \n",
    "            Xtrain_sample = Xtrain[np.random.randint(0,Xtrain.shape[0],batch_size)]\n",
    "            Z = np.random.normal(0,1,(batch_size,z_dim))      \n",
    "            Fake_Data_by_GenerativeNet = model.Gen.predict(Z)\n",
    "            \n",
    "            Dis_loss_Real = model.Dis.train_on_batch(Xtrain_sample,Real_ideal)\n",
    "            Dis_loss_Fake = model.Dis.train_on_batch(Fake_Data_by_GenerativeNet,Fake_ideal)\n",
    "        \n",
    "            Dis_loss, Dis_accuracy = 0.5*(np.add(Dis_loss_Real,Dis_loss_Fake))\n",
    "        \n",
    "        \n",
    "        ## Generative Network Training\n",
    "        Z = np.random.normal(0,1,(batch_size,z_dim))\n",
    "        Gen_loss = model.GAN.train_on_batch(Z,Fake_ideal)\n",
    "        \n",
    "        if (Iter+1) % Interval == 0:\n",
    "            Info.losses.append((Dis_loss,Gen_loss))\n",
    "            Info.accuracies.append(np.multiply(100,Dis_accuracy))\n",
    "            Info.iteration_checks.append(Iter+1)\n",
    "            \n",
    "            print(\"%d [Dis_loss: %f , Dis_accuracy: %.2f] [Gen_loss: %f]\" %\n",
    "                  (Iter+1,Dis_loss,np.multiply(100.0,Dis_accuracy),Gen_loss))            \n",
    "            \n",
    "            show_results(model,z_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91d4bc",
   "metadata": {},
   "source": [
    "## Wasserstein Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e68beb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wasserstein_loss (y_True, y_Pred):\n",
    "    return backend.mean(y_True*y_Pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b115020",
   "metadata": {},
   "source": [
    "## Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b38c4783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10548\\3864481572.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_Dis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m model.Dis.compile (loss = Wasserstein_loss,\n\u001b[0;32m     18\u001b[0m                \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0002\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10548\\836216490.py\u001b[0m in \u001b[0;36mbuild_Dis\u001b[1;34m(self, data_size)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'same'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pouya\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                     \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                     \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pouya\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pouya\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pouya\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                 \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                 raise ValueError('Layer ' + self.name + ' was called with '\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pouya\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m     \"\"\"\n\u001b[1;32m--> 695\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    696\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[0;32m    697\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pouya\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mis_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_TensorLike\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dense_tensor_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'"
     ]
    }
   ],
   "source": [
    "class info:\n",
    "    \n",
    "    def __init__(self,losses=[],accuracies=[],iteration_checks=[]):\n",
    "        \n",
    "        self.losses = losses\n",
    "        self.accuracies = accuracies\n",
    "        self.iteration_checks = iteration_checks\n",
    "        \n",
    "Xtrain = Input_Data()\n",
    "data_size = Xtrain.shape[1:]\n",
    "batch_size = 64\n",
    "z_dim = 100\n",
    "\n",
    "model  = GAN()\n",
    "\n",
    "model.build_Dis(data_size)\n",
    "model.Dis.compile (loss = Wasserstein_loss,\n",
    "               optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2 = 0.999),\n",
    "               metrics = ['accuracy'])\n",
    "\n",
    "model.build_Gen(z_dim)\n",
    "# model.Gen.compile(loss = 'binary_crossentropy',\n",
    "#                optimizer = Adam())\n",
    "\n",
    "model.Dis.trainable = False\n",
    "\n",
    "model.build_GAN()\n",
    "model.GAN.compile(loss = Wasserstein_loss,\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2 = 0.999))\n",
    "\n",
    "    \n",
    "Info = info([],[],[])\n",
    "Train(batch_size, Xtrain, z_dim, Info, Iters = 20000, Interval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb5e93c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\adria\\anaconda3\\envs\\pouya\\lib\\site-packages (1.7.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\adria\\anaconda3\\envs\\pouya\\lib\\site-packages (from scipy) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\adria\\anaconda3\\envs\\pouya\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7777d55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
